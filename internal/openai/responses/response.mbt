///|
/// A full response object returned by the Responses API.
pub struct Response {
  /// Unique identifier for this response.
  id : String
  /// Unix timestamp (in seconds) of when this response was created.
  created_at : Double
  /// An error object returned when the model fails to generate a response.
  error : ResponseError
  /// Details about why the response is incomplete.
  incomplete_details : ResponseIncompleteDetails
  /// A system (or developer) message inserted into the model's context.
  instructions : ResponseInstructions
  /// Metadata attached to the response.
  metadata : @shared.Metadata
  /// Model ID used to generate the response.
  model : @shared.ResponsesModel
  /// The object type of this resource - always set to `response`.
  object_ : String
  /// An array of output items generated by the model.
  output : Array[ResponseOutputItem]
  /// Whether to allow the model to run tool calls in parallel.
  parallel_tool_calls : Bool
  /// Sampling temperature between 0 and 2.
  temperature : Double
  /// How the model should select which tool to use.
  tool_choice : ResponseToolChoice
  /// Tools the model may call while generating a response.
  tools : Array[Tool]
  /// Nucleus sampling parameter.
  top_p : Double
  /// Whether to run the model response in the background.
  background : Bool?
  /// The conversation that this response belongs to.
  conversation : ResponseConversation?
  /// Upper bound for the number of tokens that can be generated.
  max_output_tokens : Int64?
  /// Maximum number of total calls to built-in tools.
  max_tool_calls : Int64?
  /// The unique ID of the previous response.
  previous_response_id : String?
  /// Reference to a prompt template and its variables.
  prompt : ResponsePrompt?
  /// Prompt cache key for caching.
  prompt_cache_key : String
  /// Retention policy for the prompt cache.
  prompt_cache_retention : ResponsePromptCacheRetention?
  /// Configuration options for reasoning models.
  reasoning : @shared.Reasoning?
  /// Stable identifier to help detect policy-violating users.
  safety_identifier : String
  /// Processing tier used for serving the request.
  service_tier : ResponseServiceTier?
  /// Status of the response generation.
  status : ResponseStatus
  /// Configuration options for a text response from the model.
  text : ResponseTextConfig
  /// Number of most likely tokens to return at each token position.
  top_logprobs : Int64?
  /// Truncation strategy used for the model response.
  truncation : ResponseTruncation?
  /// Token usage details.
  usage : ResponseUsage
  /// Deprecated user identifier.
  user : String?
}

///|
pub impl ToJson for Response with to_json(self : Response) -> Json {
  let obj : Map[String, Json] = {
    "id": self.id,
    "created_at": self.created_at,
    "error": self.error,
    "incomplete_details": self.incomplete_details,
    "instructions": self.instructions,
    "metadata": self.metadata,
    "model": self.model,
    "object": self.object_,
    "output": self.output,
    "parallel_tool_calls": self.parallel_tool_calls,
    "temperature": self.temperature,
    "tool_choice": self.tool_choice,
    "tools": self.tools,
    "top_p": self.top_p,
    "prompt_cache_key": self.prompt_cache_key,
    "safety_identifier": self.safety_identifier,
    "status": self.status,
    "text": self.text,
    "usage": self.usage,
  }
  if self.background is Some(background) {
    obj["background"] = background.to_json()
  }
  if self.conversation is Some(conversation) {
    obj["conversation"] = conversation.to_json()
  }
  if self.max_output_tokens is Some(max_output_tokens) {
    obj["max_output_tokens"] = @jsonx.int64(max_output_tokens)
  }
  if self.max_tool_calls is Some(max_tool_calls) {
    obj["max_tool_calls"] = @jsonx.int64(max_tool_calls)
  }
  if self.previous_response_id is Some(previous_response_id) {
    obj["previous_response_id"] = previous_response_id.to_json()
  }
  if self.prompt is Some(prompt) {
    obj["prompt"] = prompt.to_json()
  }
  if self.prompt_cache_retention is Some(prompt_cache_retention) {
    obj["prompt_cache_retention"] = prompt_cache_retention.to_json()
  }
  if self.reasoning is Some(reasoning) {
    obj["reasoning"] = reasoning.to_json()
  }
  if self.service_tier is Some(service_tier) {
    obj["service_tier"] = service_tier.to_json()
  }
  if self.top_logprobs is Some(top_logprobs) {
    obj["top_logprobs"] = @jsonx.int64(top_logprobs)
  }
  if self.truncation is Some(truncation) {
    obj["truncation"] = truncation.to_json()
  }
  if self.user is Some(user) {
    obj["user"] = user.to_json()
  }
  Json::object(obj)
}

///|
pub impl @json.FromJson for Response with from_json(
  json : Json,
  path : @json.JsonPath,
) -> Response raise @json.JsonDecodeError {
  let object = @jsonx.as_object(json, path~)
  let id : String = object.required("id", path~)
  let created_at : Double = object.required("created_at", path~)
  let error : ResponseError = object.required("error", path~)
  let incomplete_details : ResponseIncompleteDetails = object.required(
    "incomplete_details",
    path~,
  )
  let instructions : ResponseInstructions = object.required(
    "instructions",
    path~,
  )
  let metadata : @shared.Metadata = object.required("metadata", path~)
  let model : @shared.ResponsesModel = object.required("model", path~)
  let object_ : String = object.required("object", path~)
  let output : Array[ResponseOutputItem] = object.required("output", path~)
  let parallel_tool_calls : Bool = object.required("parallel_tool_calls", path~)
  let temperature : Double = object.required("temperature", path~)
  let tool_choice : ResponseToolChoice = object.required("tool_choice", path~)
  let tools : Array[Tool] = object.required("tools", path~)
  let top_p : Double = object.required("top_p", path~)
  let background : Bool? = object.optional("background", path~)
  let conversation : ResponseConversation? = object.optional(
    "conversation",
    path~,
  )
  let max_output_tokens = object.optional_int64("max_output_tokens", path~)
  let max_tool_calls = object.optional_int64("max_tool_calls", path~)
  let previous_response_id : String? = object.optional(
    "previous_response_id",
    path~,
  )
  let prompt : ResponsePrompt? = object.optional("prompt", path~)
  let prompt_cache_key : String = object.required("prompt_cache_key", path~)
  let prompt_cache_retention : ResponsePromptCacheRetention? = object.optional(
    "prompt_cache_retention",
    path~,
  )
  let reasoning : @shared.Reasoning? = object.optional("reasoning", path~)
  let safety_identifier : String = object.required("safety_identifier", path~)
  let service_tier : ResponseServiceTier? = object.optional(
    "service_tier",
    path~,
  )
  let status : ResponseStatus = object.required("status", path~)
  let text : ResponseTextConfig = object.required("text", path~)
  let top_logprobs = object.optional_int64("top_logprobs", path~)
  let truncation : ResponseTruncation? = object.optional("truncation", path~)
  let usage : ResponseUsage = object.required("usage", path~)
  let user : String? = object.optional("user", path~)
  Response::{
    id,
    created_at,
    error,
    incomplete_details,
    instructions,
    metadata,
    model,
    object_,
    output,
    parallel_tool_calls,
    temperature,
    tool_choice,
    tools,
    top_p,
    background,
    conversation,
    max_output_tokens,
    max_tool_calls,
    previous_response_id,
    prompt,
    prompt_cache_key,
    prompt_cache_retention,
    reasoning,
    safety_identifier,
    service_tier,
    status,
    text,
    top_logprobs,
    truncation,
    usage,
    user,
  }
}
